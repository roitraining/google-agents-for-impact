{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Agents for Impact: Medical Diet Navigator Challenge\n",
        "\n",
        "## Step 1\n",
        "\n",
        "This notebook is used to import the USDA Food Data Central Database into BigQuery. It does the following:\n",
        "\n",
        "1. Downloads and unzips the USDA data file (CSV) located here: https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_foundation_food_csv_2025-04-24.zip\n",
        "\n",
        "2. Creates a dataset and loads the data from the CSV files into BigQuery tables.\n",
        "\n",
        "The data is used by the ADK BigQuery Agent (configured in Step 2) to retrieve food and nutrition information.\n"
      ],
      "metadata": {
        "id": "fBL9dlBHzAM0"
      },
      "id": "fBL9dlBHzAM0"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Project and Vertex AI\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# define project information manually if the above code didn't work\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "\n",
        "print(PROJECT_ID)\n",
        "\n",
        "STAGING_BUCKET = \"gs://dsf345gjt\"\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n"
      ],
      "metadata": {
        "id": "xg6HAyzqSstq"
      },
      "id": "xg6HAyzqSstq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Authenticate to BQ\n",
        "\n",
        "import google.auth\n",
        "from google.cloud import bigquery\n",
        "creds, proj = google.auth.default()\n",
        "print(\"ADC project:\", proj)\n",
        "\n",
        "# This just tests to ensure we are authenticated to BQ\n",
        "bq = bigquery.Client(project=PROJECT_ID, credentials=creds)\n",
        "bq.query(\"SELECT 1\").result()\n",
        "print(\"BigQuery query OK\")"
      ],
      "metadata": {
        "id": "wi_HQeTKGDy3"
      },
      "id": "wi_HQeTKGDy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "ZzCZtFUfR8FBFuhQqx2TCWG9",
      "metadata": {
        "tags": [],
        "id": "ZzCZtFUfR8FBFuhQqx2TCWG9"
      },
      "source": [
        "# @title Set BigQuery Variables\n",
        "DATASET_NAME = \"usda_dataset\"\n",
        "DOWNLOAD_URL = \"https://fdc.nal.usda.gov/fdc-datasets/FoodData_Central_foundation_food_csv_2025-04-24.zip\"\n",
        "EXTRACT_DIR = \"fooddata_csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download and Unzip Data File\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# --- Config ---\n",
        "url = DOWNLOAD_URL\n",
        "zip_filename = \"fooddata.zip\"\n",
        "extract_dir = EXTRACT_DIR  # expects EXTRACT_DIR to be defined in a previous cell\n",
        "\n",
        "# --- Always start from scratch ---\n",
        "if os.path.exists(zip_filename):\n",
        "    try:\n",
        "        os.remove(zip_filename)\n",
        "        print(f\"Removed existing zip: {zip_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not remove existing zip ({zip_filename}): {e}\")\n",
        "\n",
        "if os.path.isdir(extract_dir):\n",
        "    try:\n",
        "        shutil.rmtree(extract_dir)\n",
        "        print(f\"Removed existing directory: {extract_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not remove existing directory ({extract_dir}): {e}\")\n",
        "\n",
        "# --- Download fresh zip ---\n",
        "print(\"Downloading dataset...\")\n",
        "with requests.get(url, stream=True, timeout=60) as response:\n",
        "    response.raise_for_status()\n",
        "    with open(zip_filename, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1 MB chunks\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# --- Extract into a fresh directory ---\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# --- List all extracted files ---\n",
        "print(\"Extracted files:\")\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    for filename in sorted(files):\n",
        "        print(os.path.join(root, filename))\n"
      ],
      "metadata": {
        "id": "CnLeDfRzsKg7"
      },
      "id": "CnLeDfRzsKg7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save CSV files to BigQuery\n",
        "import os\n",
        "import glob\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound, GoogleAPICallError, BadRequest\n",
        "\n",
        "# Where you extracted the USDA CSVs (expects EXTRACT_DIR set earlier)\n",
        "BASE_DIR = EXTRACT_DIR\n",
        "MAX_BAD_RECORDS = 1000  # <-- allow up to 100 problematic rows per file\n",
        "\n",
        "# Try to find the inner folder automatically (e.g., FoodData_Central_foundation_food_csv_2025-04-24)\n",
        "subdirs = [d for d in glob.glob(os.path.join(BASE_DIR, \"*\")) if os.path.isdir(d)]\n",
        "DATA_DIR = subdirs[0] if subdirs else BASE_DIR\n",
        "\n",
        "print(\"Using data directory:\", DATA_DIR)\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "dataset_ref = bigquery.DatasetReference(PROJECT_ID, DATASET_NAME)\n",
        "\n",
        "# Ensure the dataset exists (create if it doesn't)\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(f\"Dataset {PROJECT_ID}.{DATASET_NAME} exists.\")\n",
        "except NotFound:\n",
        "    ds = bigquery.Dataset(dataset_ref)\n",
        "    ds.location = \"US\"  # change if needed\n",
        "    client.create_dataset(ds)\n",
        "    print(f\"Created dataset {PROJECT_ID}.{DATASET_NAME} in location {ds.location}.\")\n",
        "\n",
        "# Load config for CSVs\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    autodetect=True,                         # Let BigQuery infer schema\n",
        "    skip_leading_rows=1,                     # Header row\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "    field_delimiter=\",\",\n",
        "    quote_character='\"',\n",
        "    allow_quoted_newlines=True,\n",
        "    encoding=\"UTF-8\",\n",
        "    max_bad_records=MAX_BAD_RECORDS,         # <-- tolerate up to N bad rows\n",
        "    ignore_unknown_values=True,            # (Optional) tolerate extra columns\n",
        ")\n",
        "\n",
        "# Iterate over all CSV files and load each into a table named after the file (minus .csv)\n",
        "csv_paths = sorted(glob.glob(os.path.join(DATA_DIR, \"*.csv\")))\n",
        "if not csv_paths:\n",
        "    raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}\")\n",
        "\n",
        "summary = []\n",
        "failures = []\n",
        "\n",
        "for csv_path in csv_paths:\n",
        "    table_name = os.path.splitext(os.path.basename(csv_path))[0]\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "    print(f\"\\nLoading {csv_path} -> {PROJECT_ID}.{DATASET_NAME}.{table_name}\")\n",
        "\n",
        "    try:\n",
        "        with open(csv_path, \"rb\") as f:\n",
        "            load_job = client.load_table_from_file(f, table_ref, job_config=job_config)\n",
        "\n",
        "        # Wait for completion; raises on job failure\n",
        "        load_job.result()\n",
        "\n",
        "        # Job may still have partial errors (within max_bad_records)\n",
        "        if load_job.errors:\n",
        "            print(f\"Completed with {len(load_job.errors)} load errors (tolerated by max_bad_records={MAX_BAD_RECORDS}).\")\n",
        "            for err in load_job.errors[:5]:\n",
        "                print(\"  -\", err)\n",
        "\n",
        "        table = client.get_table(table_ref)\n",
        "        print(f\"Loaded {table.num_rows} rows into {table.full_table_id}\")\n",
        "        summary.append((table_name, table.num_rows))\n",
        "\n",
        "    except (BadRequest, GoogleAPICallError, Exception) as e:\n",
        "        # Try to surface job-level error details if available\n",
        "        err_details = None\n",
        "        try:\n",
        "            if 'load_job' in locals() and load_job is not None:\n",
        "                if getattr(load_job, \"error_result\", None):\n",
        "                    err_details = load_job.error_result\n",
        "                elif getattr(load_job, \"errors\", None):\n",
        "                    err_details = load_job.errors[:5]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(f\"ERROR loading {table_name}: {e}\")\n",
        "        if err_details:\n",
        "            print(\"  Details:\", err_details)\n",
        "        failures.append((table_name, str(e)))\n",
        "\n",
        "print(\"\\n=== Load Summary ===\")\n",
        "for name, rows in summary:\n",
        "    print(f\"{name}: {rows} rows\")\n",
        "\n",
        "if failures:\n",
        "    print(\"\\n=== Failures ===\")\n",
        "    for name, msg in failures:\n",
        "        print(f\"{name}: {msg}\")\n",
        "else:\n",
        "    print(\"\\nNo failures ðŸŽ‰\")\n"
      ],
      "metadata": {
        "id": "yX_z9YuP4qKm"
      },
      "id": "yX_z9YuP4qKm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Step_1_USDA_Data_Import_BQ.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}